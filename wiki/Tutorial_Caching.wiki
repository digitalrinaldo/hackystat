#summary Caching in Hackystat

== 1.0 Motivation ==

In Hackystat Versions 1 to 7, the system provided a standard two-tier client server architecture:


[http://hackystat.googlecode.com/svn/wiki/caching.1.gif]

This architecture was simple, and as long as the system remained simple, it
worked quite well.  Unfortunately, by Version 7, the system had become
quite complex, with many different extension points, and we recognized that
in order to better support future development, we should decompose the "server" into
a set of services communicating via HTTP:

[http://hackystat.googlecode.com/svn/wiki/caching.2.gif]

This architecture is in many ways a huge improvement.  For example, if you want to build a new Hackystat feature called "Foo" that depends only on, say, !DailyProjectData, you can communicate with that service directly through HTTP:

[http://hackystat.googlecode.com/svn/wiki/caching.3.gif]

You can write your Foo feature using virtually any language or framework you want, since it communicates with Hackystat through HTTP. 

The new architecture is also more scalable.  Let's say that the web interface is getting bogged down.  Well, just bring up a second one on a different machine and tell some of your users to go there instead:

[http://hackystat.googlecode.com/svn/wiki/caching.4.gif]

Basically any of the services can be replicated, leading to all sorts of different distributed architectures. 

While the "service-oriented architecture" in Hackystat 8 has a number of strengths, it also has one substantial weakness: what used to be accomplished with a single HTTP request in the old client-server architecture might now require dozens, hundreds, or even thousands of HTTP requests!  

To see why this is so, let's consider an important Hackystat use case:  the Project Portfolio analysis:

[http://hackystat.googlecode.com/svn/wiki/caching.5.gif]

In general, each histogram/sparkline in the portfolio analysis corresponds to one telemetry chart providing trend data for a certain period of time. In the above display, the analysis is showing ten projects and ten trend lines, where each trend line corresponds to five weeks.   

Let's look at how that translates into HTTP calls.  To start, let's watch the first project attempt to get the first historgram. It begins by the user in their browser requesting the portfolio page from the web interface.  To build it, the web interface requests the trend data for the first analysis (Coverage) on the first project (hackystat-analysis-dailyprojectdata):

[http://hackystat.googlecode.com/svn/wiki/caching.6.gif]

So far, not bad: just two HTTP requests.  Now, to build that trend data, the telemetry service needs to get some !DailyProjectData instances.  In fact, it needs to get one DPD instance per day for the analysis interval.  I said that this analysis was for five weeks, so that's 35 days, or 35 HTTP request/response cycles to the !DailyProjectData service for that single trend:

[http://hackystat.googlecode.com/svn/wiki/caching.7.gif]

Not good, but it gets worse:  for each of these 35 !DailyProjectData instances, the service might needs to retrieve dozens to hundreds of sensor data instances from the !SensorBase:

[http://hackystat.googlecode.com/svn/wiki/caching.8.gif]

Let's assume each DPD instance requires 20 sensor data instances on average.  That means that for just one of the trend lines in the portfolio analysis, we have to get 1 (web interface) + 1 (Telemetry) + 35 (!DailyProjectData) + 700 (!SensorBase) = 737 HTTP requests.

Unfortunately, for the entire portfolio analysis shown above, we have to get ten trend lines for each of ten projects, so we actually have 100 trend lines to retrieve.  That means that this single simple portfolio analysis could require 10 `*` 10 `*` 737 `=` 73,700 HTTP requests.  Holy Smokes! 

== 2.0 Caching to the rescue ==

Fortunately, one of the beautiful things about the [http://en.wikipedia.org/wiki/Representational_State_Transfer RESTful architecture] used in Hackystat 8 is that it is well suited to caching.  Basically, the Telemetry service caches any !DailyProjectData instances it requests from the !DailyProjectData service.  Similarly, the !DailyProjectData service caches any !SensorData instances it requests from the !SensorBase. 

What this means is the following: the first time anyone in a project requests data, they have to pay the "full price" in terms of HTTP requests, but from that point on, everyone else can exploit the cached data to eliminate most of the computation and HTTP requests.   

Hackystat is well suited to this approach, since any given project member will typically run an analysis like Portfolio every few days over the past several weeks of data.  That means that most of the trend data will have been computed previously and be stored in one of the caches.  In the best case, all of it will have been previously cached, resulting in just 100 HTTP requests:

[http://hackystat.googlecode.com/svn/wiki/caching.9.gif]

If each HTTP request takes about a half second, so that's about 50 seconds to display the above portfolio analysis, which is displaying trends in 10 measures for 10 projects for 35 days, or about 3,500 data points.  

As you might expect, we could easily reduce the number of HTTP requests to 1 (for this best case scenario where all of the computation has been done previously) simply by adding a cache to the web interface service

[http://hackystat.googlecode.com/svn/wiki/caching.10.gif]

However, we have not yet felt the need to do this. 

== 3.0 The problem with caching ==























